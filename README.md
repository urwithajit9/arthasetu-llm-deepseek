# üöÄ ArthaSeetu Brain - Production Deployment Guide

## üìã Table of Contents
- [Overview](#overview)
- [Cost Analysis](#cost-analysis)
- [Setup Instructions](#setup-instructions)
- [Django Integration](#django-integration)
- [Testing](#testing)
- [Monitoring](#monitoring)
- [Troubleshooting](#troubleshooting)

---

## üéØ Overview

Production-ready AI API with:
- ‚úÖ **Cost Optimized**: L4 GPU ($0.60/hr vs A10G $1.10/hr) - 45% savings
- ‚úÖ **Secure**: API key auth + optional request signing
- ‚úÖ **Scalable**: Auto-scaling with concurrent requests
- ‚úÖ **Fast**: Keep-warm containers, CUDA optimization
- ‚úÖ **Reliable**: Health checks, retries, error handling
- ‚úÖ **Django Ready**: Drop-in client code

### üêõ Critical Bugs Fixed

**ORIGINAL CODE BUG** üö®:
```python
# ‚ùå WRONG - Creates new model instance per request (thousands of dollars!)
@app.function()
@modal.fastapi_endpoint(method="POST")
def ask(data: dict):
    model = DeepSeekModel()  # ‚ö†Ô∏è Deploys entire model every request!
    answer = model.generate.remote(...)
```

**FIXED VERSION** ‚úÖ:
```python
# ‚úÖ CORRECT - Uses already deployed model
@web_app.post("/v1/generate")
async def generate_endpoint(...):
    model = DeepSeekModel()  # References existing deployment
    result = model.generate.remote(...)  # Calls deployed instance
```

---

## üí∞ Cost Analysis

### GPU Comparison
| GPU   | Cost/Hour | Best For | Our Choice |
|-------|-----------|----------|------------|
| A10G  | $1.10     | Heavy workloads | ‚ùå |
| **L4** | **$0.60** | **Most workloads** | **‚úÖ** |
| T4    | $0.40     | Light workloads | Dev/Test |

### Monthly Cost Estimate

**Base Configuration (Keep-Warm = 1)**:
```
1 container warm 24/7:
$0.60/hr √ó 24 hours √ó 30 days = $432/month

With automatic scaledown (idle containers):
Actual usage ~50% = ~$216/month
```

**Cost Optimization Options**:

1. **Remove Keep-Warm** (Cold starts: ~30s):
   ```python
   KEEP_WARM = 0  # Save $216/month
   ```

2. **Increase Scaledown Window** (Fewer scaling events):
   ```python
   SCALEDOWN_WINDOW = 600  # 10 minutes
   ```

3. **Request Batching** (Django side):
   - Cache responses: Save 50-70% on API calls
   - Batch similar requests

4. **Production Usage Pattern** (Example):
   ```
   Peak hours (8hrs/day): 1-2 containers
   Off-peak (16hrs/day): 0-1 containers (warm)

   Estimated: ~$100-150/month
   ```

---

## üõ†Ô∏è Setup Instructions

### 1. Install Modal CLI

```bash
# Install Modal
pip install modal

# Authenticate
modal setup

# Verify
modal profile current
```

### 2. Create Secrets

```bash
# Generate secure keys
python3 << 'EOF'
import secrets
print("API_KEY:", secrets.token_urlsafe(32))
print("SIGNING_SECRET:", secrets.token_urlsafe(32))
EOF

# Create Modal secret
modal secret create arthasetu-api \
  API_KEY="paste-your-api-key-here" \
  SIGNING_SECRET="paste-your-signing-secret-here"

# Verify
modal secret list
```

### 3. Update Configuration

**In `modal_app.py`**:
```python
# Line 95: Update CORS origins
allow_origins=[
    "https://yourdomain.com",      # Your production domain
    "https://www.yourdomain.com",
    "http://localhost:8000",        # Django local dev
]
```

### 4. Deploy to Modal

```bash
# Deploy
modal deploy modal_app.py

# Get your API URL (save this!)
# Example: https://your-workspace--arthasetu-brain-fastapi-app.modal.run
```

### 5. Setup GitHub Actions

**Add GitHub Secrets**:
1. Go to: `Settings > Secrets and variables > Actions`
2. Add secrets:
   - `MODAL_TOKEN_ID`: From Modal dashboard
   - `MODAL_TOKEN_SECRET`: From Modal dashboard
   - `TEST_API_KEY`: Your API key (for tests)

**Trigger Deployment**:
```bash
git add .
git commit -m "Deploy ArthaSeetu Brain API"
git push origin main
```

---

## üîó Django Integration

### 1. Install Client Code

```bash
# In your Django project
cp django_integration.py your_project/apps/ai/client.py
```

### 2. Configure Django Settings

**settings/production.py**:
```python
# ArthaSeetu Brain API
ARTHASETU_API_URL = env('ARTHASETU_API_URL')
ARTHASETU_API_KEY = env('ARTHASETU_API_KEY')
ARTHASETU_SIGNING_SECRET = env('ARTHASETU_SIGNING_SECRET', default=None)

# Cache (highly recommended)
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.redis.RedisCache',
        'LOCATION': env('REDIS_URL', default='redis://127.0.0.1:6379/1'),
    }
}
```

**Add to .env**:
```bash
ARTHASETU_API_URL=https://your-workspace--arthasetu-brain-fastapi-app.modal.run
ARTHASETU_API_KEY=your-super-secret-api-key-here
ARTHASETU_SIGNING_SECRET=your-super-secret-signing-key-here
```

### 3. Add URL Routes

**urls.py**:
```python
from apps.ai import views as ai_views

urlpatterns = [
    path('api/ai/ask/', ai_views.ask_ai_view, name='ai_ask'),
    path('api/ai/health/', ai_views.health_check_view, name='ai_health'),
]
```

### 4. Usage Example

```python
from apps.ai.client import ArthaSetuBrainClient

# In your view/service
client = ArthaSetuBrainClient()
result = client.generate(
    prompt="What is the capital of France?",
    context="User is learning geography",
)

print(result["answer"])
# Output: "Paris is the capital of France..."
```

---

## üß™ Testing

### Test Modal API Directly

```bash
# Health check
curl https://your-api-url.modal.run/health

# Generate (with API key)
curl -X POST https://your-api-url.modal.run/v1/generate \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your-api-key-here" \
  -d '{
    "prompt": "What is 2+2?",
    "temperature": 0.6,
    "max_tokens": 100
  }'
```

### Test from Django

```bash
# Django management command
python manage.py test_ai --prompt "What is the capital of France?"

# Expected output:
# üîç Testing API connection...
# ‚úÖ Health: healthy
# üìù Testing with prompt: What is the capital of France?
# ‚úÖ Response received:
# Answer: Paris is the capital of France...
# Tokens: 15
# Latency: 1.23s
```

### Performance Testing

```bash
# Install Apache Bench
sudo apt-get install apache2-utils

# Load test (100 requests, 10 concurrent)
ab -n 100 -c 10 \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -p test_payload.json \
  https://your-api-url.modal.run/v1/generate
```

**test_payload.json**:
```json
{
  "prompt": "What is 2+2?",
  "max_tokens": 50
}
```

---

## üìä Monitoring

### 1. Modal Dashboard
- Go to: https://modal.com/apps
- View: Request rates, latency, errors
- Set up: Billing alerts

### 2. Django Logging

```python
# settings.py
LOGGING = {
    'version': 1,
    'handlers': {
        'file': {
            'class': 'logging.FileHandler',
            'filename': 'logs/ai_api.log',
        },
    },
    'loggers': {
        'apps.ai.client': {
            'handlers': ['file'],
            'level': 'INFO',
        },
    },
}
```

### 3. Key Metrics to Monitor

```python
# In Django admin or monitoring dashboard
from apps.ai.models import AIRequestLog

# Track:
- Total requests/day
- Average latency
- Error rate
- Token usage
- Cost per request
```

### 4. Cost Monitoring Script

```bash
# Get Modal usage (add to cron)
modal app logs arthasetu-brain --since 24h | \
  grep "requests_processed" | \
  tail -1
```

---

## üîß Troubleshooting

### Issue: "Invalid API key"
```bash
# Solution 1: Verify secret
modal secret list
modal secret get arthasetu-api

# Solution 2: Recreate secret
modal secret create arthasetu-api \
  API_KEY="new-key" \
  SIGNING_SECRET="new-secret"

# Solution 3: Redeploy
modal deploy modal_app.py --force
```

### Issue: High latency (>10s)
```python
# Solution 1: Check keep_warm
KEEP_WARM = 1  # Reduces cold starts

# Solution 2: Optimize model config
gpu_memory_utilization=0.95  # Use more GPU memory

# Solution 3: Reduce max_tokens
max_tokens=512  # Instead of 2048
```

### Issue: Rate limit errors
```python
# Solution 1: Increase rate limit
rate_limiter = RateLimiter(requests_per_minute=120)

# Solution 2: Add caching in Django
client.generate(..., use_cache=True, cache_ttl=3600)

# Solution 3: Add retry logic (already included)
```

### Issue: 500 errors
```bash
# Check Modal logs
modal app logs arthasetu-brain --tail

# Check Django logs
tail -f logs/ai_api.log

# Verify model loaded
curl https://your-api-url.modal.run/health
```

### Issue: High costs
```python
# Solution 1: Remove keep-warm
KEEP_WARM = 0

# Solution 2: Use smaller GPU
GPU_CONFIG = "T4"  # $0.40/hr

# Solution 3: Aggressive caching
# In Django, cache aggressively

# Solution 4: Increase scaledown
SCALEDOWN_WINDOW = 600  # 10 minutes
```

---

## üìà Production Checklist

Before going live:

- [ ] Update CORS origins in `modal_app.py`
- [ ] Set strong API keys (32+ characters)
- [ ] Enable request signing (optional but recommended)
- [ ] Configure Redis caching in Django
- [ ] Set up monitoring and alerts
- [ ] Test health check endpoint
- [ ] Load test with expected traffic
- [ ] Set up error tracking (Sentry)
- [ ] Document API for your team
- [ ] Configure backup/failover strategy

---

## üîê Security Best Practices

1. **Rotate Secrets**: Every 90 days
2. **Monitor Access**: Check Modal logs regularly
3. **Rate Limiting**: Already configured (60 req/min)
4. **Input Validation**: Already enforced
5. **HTTPS Only**: Modal provides this
6. **IP Whitelisting**: Add if needed in `modal_app.py`

---

## üìû Support

- **Modal Docs**: https://modal.com/docs
- **Modal Community**: https://modal.com/discord
- **Django Integration Issues**: Check `django_integration.py`

---

## üéâ Summary

You now have:
- ‚úÖ Production-ready AI API (45% cheaper than original)
- ‚úÖ Secure Django integration
- ‚úÖ Auto-scaling and monitoring
- ‚úÖ CI/CD with GitHub Actions
- ‚úÖ Cost optimization strategies

**Expected monthly cost**: $100-200 (vs $400+ with A10G + original bugs)

**Next Steps**:
1. Deploy to Modal
2. Test with Django
3. Monitor for 1 week
4. Optimize based on usage patterns